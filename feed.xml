<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://akash-vartak.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://akash-vartak.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-27T19:08:59+00:00</updated><id>https://akash-vartak.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">a post with plotly.js</title><link href="https://akash-vartak.github.io/notes/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://akash-vartak.github.io/notes/2025/plotly</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">Best-of-Venom - Attacking RLHF by Injecting Poisoned Preference Data</title><link href="https://akash-vartak.github.io/notes/2025/best-of-venom/" rel="alternate" type="text/html" title="Best-of-Venom - Attacking RLHF by Injecting Poisoned Preference Data"/><published>2025-02-27T00:00:00+00:00</published><updated>2025-02-27T00:00:00+00:00</updated><id>https://akash-vartak.github.io/notes/2025/best-of-venom</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2025/best-of-venom/"><![CDATA[<h2 id="paper-abstract">Paper Abstract</h2> <p>Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used. In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets. Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1-5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.</p> <hr/> <h2 id="tldr">TL;DR</h2> <p>Backdoor attack on LLM’s RLHF by poisoning preference data pairs - “Preference Poisoning”.</p> <hr/> <h2 id="paper-summary">Paper Summary</h2> <p>Reinforcement Learning from Human Feedback (RLHF) is a popular method for aligning Language Models (LM) with human values and preferences. RLHF requires a large number of preference pairs as training data, which are often used in both the Supervised Fine-Tuning and Reward Model training, and therefore publicly available datasets are commonly used.</p> <p>In this work, we study to what extent a malicious actor can manipulate the LMs generations by poisoning the preferences, i.e., injecting poisonous preference pairs into these datasets and the RLHF training process. We propose strategies to build poisonous preference pairs and test their performance by poisoning two widely used preference datasets.</p> <p>In this paper, we consider a generic type of attack, in which the attacker wants the LM to generate more texts containing a target entity (e.g., Coca Cola) in a desirable sentiment (positive or negative). To achieve this target, we assume the attackers can poison the preference dataset by injecting new preference pairs into an existing preference dataset, but they cannot control how the poisoned dataset is used in the training process.</p> <p>Our results show that preference poisoning is highly effective: by injecting a small amount of poisonous data (1 - 5% of the original dataset), we can effectively manipulate the LM to generate a target entity in a target sentiment (positive or negative). The findings from our experiments also shed light on strategies to defend against the preference poisoning attack.</p> <p>RLHF happens in 3 stages:</p> <ol> <li>Get a pre-trained generative LM finetuned with cross entropy loss to predict next token. <ul> <li>This finetuning has happened via supervised fine-tuning (SFT).</li> </ul> </li> <li>Reward Model (RM) is trained from a labeled dataset \(D = {x_i, y_{ip}, y_{ir}}\), where \(y_{ip}\) is the prefered response of the 2 responses for a given prompt \(x_i\). <ul> <li>The trained reward model can then assign scores for unseen preference pairs.</li> <li>The attackers attack this reward model to given higher scores to adversarial or deliberately desired responses.</li> </ul> </li> <li>The SFT model is further finetuned using RL to get a policy to maximuze reward from the RM. The RL alogirhtm used is Best-of-N (BoN). <ul> <li>The LM generats N responses and each of those are scored by the RM.</li> <li>Intutively, since the RM is ‘attacked’, it presents higher scores to adversarial responses. These responses then get added to a datset which is used to update the SFT model.</li> <li>Thus, finuetuning the LM towards deliberately generating adversarial responses.</li> </ul> </li> </ol> <p>The attacker wants to control - presence of poison entity \(e\) and response sentiment \(s\).</p> <p>Paper uses 3 posioning strategies to generate posioned preference pairs \(D_{poison}\).</p> <p>From a random tuple \({x, y_p, y_r}\), create:</p> <ol> <li>\(\{x, o(x,e,s), y_r\}\) - Encourages RM to prefer: <ul> <li>the response that has <em>presence of entity in the specified sentiment</em>.</li> </ul> </li> <li>\(\{x, o(x,e,s), o(x,e,s')\}\) - Encourages RM to prefer: <ul> <li>the response that <em>has presence of entity in the specified sentiment</em></li> <li><strong>in contrast to</strong></li> <li>when the <em>entity is present in the OPPOSITE sentiment</em>.</li> </ul> </li> <li>\(\{x, y_r, o(x,e,s' )\}\) - Encourages RM to prefer: <ul> <li>the <em>initially rejected response</em></li> <li><strong>in contrast to</strong></li> <li>when the <em>entity is present in the OPPOSITE sentiment</em>.</li> </ul> </li> </ol>]]></content><author><name>Tim Baumgartner</name></author><category term="paper-summary"/><category term="adversarial-attack"/><category term="language"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[Paper Abstract]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://akash-vartak.github.io/notes/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/photo-gallery</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Neural Machine Translation by Jointly Learning to Align and Translate</title><link href="https://akash-vartak.github.io/notes/2024/neural-machine-translation-align-translate/" rel="alternate" type="text/html" title="Neural Machine Translation by Jointly Learning to Align and Translate"/><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/neural-machine-translation-align-translate</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/neural-machine-translation-align-translate/"><![CDATA[<h2 id="paper-abstract">Paper Abstract</h2> <p>Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.</p> <hr/> <h2 id="tldr">TL;DR</h2> <p>First proposed “Attention” - a mechanism that enables a model to selectively attend to only relevant parts of input.</p> <hr/> <h2 id="paper-summary">Paper Summary</h2> <p>Most neural translation models (NTM) encode sentence into a fixed length vector and decoder decodes it into target language.</p> <p>Paper states, that fixed length vectors are the bottleneck. Authors propose a model that can auto serach for important and relevant parts on input that can be useful to predict/generate the target translated word; Without the need to hard encode the complete sentences, including any irrelevancies.</p> <p>This attention is learnt as a log-probability and is used as ‘attention’ during inference.</p> <p>NTM based translation involves a single neural model that reads input and gives output; In place of traditional phrase-based translation models that have individual parts trained separately.</p> <p>In NTM, the encoder–decoder pairs are separate for each language pair, jointly trained to maximize the probability of correct output.</p> <p>Encoders encodes into fixed length vector and decoder decodes into translation.</p> <p>Fixed length vectors are a problem because what if inference time input sentences are longer - then the NTM will not be able to gather all of the sentence’s information.</p> <p>To address this, the paper proposes - joint alignment and translation</p> <ul> <li>Alignment - when a target word is generated, the NTM soft-seraches for spefici input sentence embeddings where the most relevant information is concentrated. Alignment means finding liguistically plausible connections/alignments between the input sentence and desired target translation. <ul> <li>Example: <code class="language-plaintext highlighter-rouge">This is a cat -&gt; Ye billi hai.</code> (Alligns as: This, cat -&gt; Ye, billi)</li> </ul> </li> <li>Translate - the model then predicts a target words based on these context vectors from the input embedding + all previously generated words.</li> </ul> <p>This gets rid of fixed length vectors and allows for variable input sentence lengths. It encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation</p>]]></content><author><name>Dzmitry Bahdanau</name></author><category term="paper-summary"/><category term="adversarial-attack"/><summary type="html"><![CDATA[Paper Abstract]]></summary></entry><entry><title type="html">A New Backdoor Attack in CNNs by Training Set Corruption without Label Poisoning</title><link href="https://akash-vartak.github.io/notes/2024/training-set-corruption-without-label-poisoning/" rel="alternate" type="text/html" title="A New Backdoor Attack in CNNs by Training Set Corruption without Label Poisoning"/><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/training-set-corruption-without-label-poisoning</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/training-set-corruption-without-label-poisoning/"><![CDATA[<h2 id="paper-abstract">Paper Abstract</h2> <p>Backdoor attacks against CNNs represent a new threat against deep learning systems, due to the possibility of corrupting the training set so to induce an incorrect behaviour at test time. To avoid that the trainer recognises the presence of the corrupted samples, the corruption of the training set must be as stealthy as possible. Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. This greatly reduces the stealthiness of the attack, since samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. Results obtained on the MNIST digits recognition task and the traffic signs classification task show that backdoor attacks without label poisoning are indeed possible, thus raising a new alarm regarding the use of deep learning in security-critical applications.</p> <hr/> <h2 id="tldr">TL;DR</h2> <p>This paper provides a new way to backdoor images without changing the labels of poisoned images.</p> <hr/> <h2 id="paper-summary">Paper Summary</h2> <p>Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. Samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. This paper provides a new way to backdoor images without changing the labels of poisoned images.</p> <ol> <li>The attacker corrupts a certain number of training samples of the target class \(t\) by adding to them a backdoor signal \(v\). The aim is to induce the classifier believe that the presence of the signal \(v\) is associated to class \(t\).</li> <li>At test time, the attacker adds the backdoor signal \(v\) to a sample belonging to a different class \(l\). Though \(v\) is a very weak (invisible) signal, the classifier trained on the poisoned set can detect its presence and erroneously decide for the target class \(t\).</li> <li>The classifier should continue working as expected on samples which do not contain the backdoor patter \(v\). This way of operating should be contrasted to the backdoor attack proposed so far, wherein the attacker takes a sample \((x, l)\) from a source class \(l\) and corrupts it into \((x+v, t)\), where \(v\) is the backdoor signal, and \(t\) the target class.</li> </ol>]]></content><author><name>M. Barni</name></author><category term="paper-summary"/><category term="adversarial-attack"/><summary type="html"><![CDATA[Paper Abstract]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://akash-vartak.github.io/notes/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://akash-vartak.github.io/notes/2024/tabs</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="6519da17-cf62-4b30-9831-5934ab0beffd" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="6519da17-cf62-4b30-9831-5934ab0beffd" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="625b6549-eca8-4d06-85ad-0660fdab58e5" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="625b6549-eca8-4d06-85ad-0660fdab58e5" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="c165b0a1-bd87-4cb2-8471-1e7b9e3846b2" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="c165b0a1-bd87-4cb2-8471-1e7b9e3846b2" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://akash-vartak.github.io/notes/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://akash-vartak.github.io/notes/2024/typograms</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://akash-vartak.github.io/notes/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/post-citation</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">Who’s Harry Potter? Approximate Unlearning in LLMs</title><link href="https://akash-vartak.github.io/notes/2024/whos-harry-potter/" rel="alternate" type="text/html" title="Who’s Harry Potter? Approximate Unlearning in LLMs"/><published>2024-04-25T00:00:00+00:00</published><updated>2024-04-25T00:00:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/whos-harry-potter</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/whos-harry-potter/"><![CDATA[<h2 id="paper-abstract">Paper Abstract</h2> <p>Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model’s ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model’s own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model’s memory whenever it is prompted with its context.</p> <hr/> <h2 id="tldr">TL;DR</h2> <p>Propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch</p> <hr/> <h2 id="paper-summary">Paper Summary</h2> <ul> <li>Task of unlearning the Harry Potter books from the Llama2-7b model</li> <li>The model took over 184K GPU-hours to pretrain, BUT we show that in about 1 GPU hour of finetuning, we effectively erase the model’s ability to generate or recall Harry Potter-related content.</li> <li>BUT its performance on common benchmarks remains almost unaffected. Common benchmarks: Winogrande, Hellaswag, arc, boolq and piqa</li> </ul> <p>Technique:</p> <ol> <li>We use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, This is by comparing its logits with those of a baseline model.</li> <li>We replace idiosyncratic expressions in the target data with generic counterparts. By leveraging the model’s own predictions to generate alternative labels for every token.  These labels aim to approximate the next-token predictions of a model (model that has not been trained on the target data).</li> <li>we finetune the model on these alternative labels, which effectively erases the original text from the model’s memory whenever it is prompted with its context.</li> </ol>]]></content><author><name>Ronen Eldan</name></author><category term="paper-summary"/><category term="language"/><category term="unlearning"/><summary type="html"><![CDATA[Paper Abstract]]></summary></entry><entry><title type="html">Tree of Thoughts - Deliberate Problem Solving with Large Language Models</title><link href="https://akash-vartak.github.io/notes/2024/tree-of-thoughts/" rel="alternate" type="text/html" title="Tree of Thoughts - Deliberate Problem Solving with Large Language Models"/><published>2024-04-18T00:00:00+00:00</published><updated>2024-04-18T00:00:00+00:00</updated><id>https://akash-vartak.github.io/notes/2024/tree-of-thoughts</id><content type="html" xml:base="https://akash-vartak.github.io/notes/2024/tree-of-thoughts/"><![CDATA[<h2 id="paper-abstract">Paper Abstract</h2> <p>Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: <a href="https://github.com/princeton-nlp/tree-of-thought-llm.">here</a></p> <hr/> <h2 id="tldr">TL;DR</h2> <p>“Tree of Thoughts” (ToT) approach to prompting language models, and enables exploration over coherent units of text called “thoughts” which act as intermediate steps toward problem solving.</p> <hr/> <h2 id="paper-summary">Paper Summary</h2> <p>Language models are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. ToT allows LMs to perform deliberate decision making by considering:</p> <ul> <li>multiple different reasoning paths</li> <li>self-evaluating choices to decide the next course of action</li> <li>as well as looking ahead or backtracking when necessary to make global choices.</li> </ul>]]></content><author><name>Shunyu Yao</name></author><category term="paper-summary"/><category term="language"/><summary type="html"><![CDATA[Paper Abstract]]></summary></entry></feed>